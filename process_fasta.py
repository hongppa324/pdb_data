import requests
import json
from tqdm import tqdm
import time

import numpy as np
import openai
from openai import OpenAI
from dotenv import load_dotenv
import os

# .envì—ì„œ API key ê°€ì ¸ì˜¤ê¸°
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

# ì €ì¥ëœ PDB ID ê¸°ë¡ íŒŒì¼ ê²½ë¡œ
PDB_LOG_FILE = "pdb_ids_svaed.json"

# pdb id ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (nê°œì”©)
def get_pdb_ids(n=100, start=0):
    # rcsb api url
    url = "https://search.rcsb.org/rcsbsearch/v2/query"

    # ìš”ì²­ query
    query = {
        "query": {
            "type": "terminal",
            "service": "text",
            "parameters": {
                "attribute": "entity_poly.rcsb_entity_polymer_type",
                "operator": "exact_match",
                "value": "Protein"
            }
        },
        "return_type": "entry",
        "request_options": {
            "paginate": {
                "start": start,
                "rows": n
            }
        }
    }

    headers = {"Content-Type": "application/json"}

    response = requests.post(url, data=json.dumps(query), headers=headers)
    if response.status_code == 200:
        result = response.json()
        return [item["identifier"] for item in result.get("result_set", [])]
    else:
        print("PDB ID ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        print(response.text)
        return []

# FASTA sequence ë‹¤ìš´ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def download_fasta(pdb_id):
    url = f"https://www.rcsb.org/fasta/entry/{pdb_id}"
    response = requests.get(url)
    if response.status_code == 200:
        return response.text
    return None

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document

# Document ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def create_documents_from_fastas(pdb_ids, log_fn=None):
    documents = []
    for i, pdb_id in enumerate(pdb_ids):
        if log_fn:
            log_fn(f"ğŸ“¥ {pdb_id} FASTA ë‹¤ìš´ë¡œë“œ ì¤‘...")
        
        fasta = download_fasta(pdb_id)
        if fasta:
            lines = fasta.strip().splitlines()
            header = lines[0].replace(">", "").strip()
            sequence = ''.join(lines[1:])

            # header parsing
            # ì˜ˆì‹œ: 101M_1|Chain A|MYOGLOBIN|Physeter catodon (9755)
            parts = header.split("|")
            if len(parts) == 4:
                sub_id = parts[0].strip()
                chain = parts[1].strip()
                protein = parts[2].strip()
                description = parts[3].strip()
            else:
                # í˜•ì‹ì´ ì•ˆ ë§ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                sub_id = header
                chain = protein = description = "Unknown"

            metadata = {
                "pdb_id": pdb_id,
                "sub_id": sub_id,
                "chain": chain,
                "protein": protein,
                "description": description,
                "sequence": sequence
            }

            doc = Document(page_content=sequence, metadata=metadata)
            documents.append(doc)

        time.sleep(0.1)
    return documents

# ì €ì¥ëœ pdb id ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
def load_saved_pdb_ids():
    if os.path.exists(PDB_LOG_FILE):
        with open(PDB_LOG_FILE, "r") as f:
            return set(json.load(f))
    return set()

# ì €ì¥ëœ pdf id ê°±ì‹ í•˜ëŠ” í•¨ìˆ˜
def saved_pdb_ids(pdb_ids_set):
    with open(PDB_LOG_FILE, "w") as f:
        json.dump(sorted(list(pdb_ids_set)), f)

# FAISS ìƒì„± ë° mergeí•˜ëŠ” í•¨ìˆ˜
def build_vectorstore(batch_size=100, total=1000, save_path="./vector_db/fasta", log_fn=print, progress_bar=None, should_continue=lambda: True):
    
    # embedding
    embeddings = OpenAIEmbeddings()

    # ì €ì¥ëœ pdb id ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    saved_ids = load_saved_pdb_ids()

    # ì´ˆê¸° vectorstore
    if os.path.exists(save_path):
        print("ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
        vectorstore = FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)
    else:
        vectorstore = None

    start_offset = len(saved_ids)
    total_batches = total // batch_size
    if progress_bar:
        progress_bar.progress(0.0)

    for i, start in enumerate(range(start_offset, start_offset + total, batch_size)):
        if not should_continue():
            log_fn("â¹ï¸ ì¤‘ë‹¨ ìš”ì²­ ê°ì§€ë¨. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break        
        
        log_fn(f"\nğŸ”„ {start} ~ {start + batch_size} PDB ID ì²˜ë¦¬ ì¤‘...")
        pdb_ids = get_pdb_ids(n=batch_size, start=start)

        # ì¤‘ë³µ ì œê±°
        new_ids = [pid for pid in pdb_ids if pid not in saved_ids]
        if not new_ids:
            log_fn("ğŸ“­ ìˆ˜ì§‘í•  ìƒˆë¡œìš´ PDB IDê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        documents = create_documents_from_fastas(new_ids, log_fn=log_fn)
        if not documents:
            log_fn("âš ï¸ ë¬¸ì„œ ì—†ìŒ. ë‹¤ìŒ ë°°ì¹˜ë¡œ.")
            continue

        new_store = FAISS.from_documents(documents, embedding=embeddings)

        if vectorstore is None:
            vectorstore = new_store
        else:
            vectorstore.merge_from(new_store)

        # ë§¤ ë°°ì¹˜ë§ˆë‹¤ ì €ì¥
        saved_ids.update(new_ids)
        saved_pdb_ids(saved_ids)

        vectorstore.save_local(save_path)
        log_fn(f"âœ… ì €ì¥ ì™„ë£Œ: {len(documents)}ê°œ ë¬¸ì„œ, ì´ ì €ì¥ëœ ID ìˆ˜: {len(saved_ids)}")

        if progress_bar:
            progress_bar.progress((i + 1) / total_batches)

    log_fn("ğŸ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")

# ì‹¤í–‰
if __name__ == "__main__":
    build_vectorstore(batch_size=100, total=1000)

# ì €ì¥ëœ vectorstore í™•ì¸í•˜ëŠ” í•¨ìˆ˜
def inspect_vectorstore(save_path="./vector_db/fasta"):
    embeddings = OpenAIEmbeddings()
    if os.path.exists(save_path):
        store = FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)

        # ë¬¸ì„œ ìˆ˜ í™•ì¸
        num_docs = len(store.docstore._dict)  # ë‚´ë¶€ dictionary ì‚¬ìš©
        print(f"ğŸ” ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ ë¬¸ì„œ ìˆ˜: {num_docs}")

        # ì¼ë¶€ ë©”íƒ€ë°ì´í„° ì¶œë ¥
        print("ğŸ“Œ ìƒ˜í”Œ ë©”íƒ€ë°ì´í„°:")
        for i, doc in enumerate(store.docstore._dict.values()):
            print(doc.metadata)
            if i >= 4:
                break
    else:
        print("âŒ ë²¡í„°ìŠ¤í† ì–´ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# í™•ì¸í•˜ëŠ” í•¨ìˆ˜ ì‹¤í–‰
if __name__ == "__main__":
    inspect_vectorstore()